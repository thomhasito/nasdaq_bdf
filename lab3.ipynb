{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student: Thomas Balsalobre Lucas Arriesse Groupe: DATA&IA 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Analysis\n",
    "# Nasdaq tech stockst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /opt/conda/lib/python3.11/site-packages (from -r requirments.txt (line 1)) (0.2.48)\n",
      "Requirement already satisfied: requests_cache in /opt/conda/lib/python3.11/site-packages (from -r requirments.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: requests_ratelimiter in /opt/conda/lib/python3.11/site-packages (from -r requirments.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: streamlit in /opt/conda/lib/python3.11/site-packages (from -r requirments.txt (line 4)) (1.39.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from -r requirments.txt (line 5)) (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from -r requirments.txt (line 6)) (3.7.1)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.11/site-packages (from -r requirments.txt (line 7)) (5.24.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (from -r requirments.txt (line 8)) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (5.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (3.5.1)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (3.17.7)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: html5lib>=1.1 in /opt/conda/lib/python3.11/site-packages (from yfinance->-r requirments.txt (line 1)) (1.1)\n",
      "Requirement already satisfied: attrs>=21.2 in /opt/conda/lib/python3.11/site-packages (from requests_cache->-r requirments.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: cattrs>=22.2 in /opt/conda/lib/python3.11/site-packages (from requests_cache->-r requirments.txt (line 2)) (24.1.2)\n",
      "Requirement already satisfied: url-normalize>=1.4 in /opt/conda/lib/python3.11/site-packages (from requests_cache->-r requirments.txt (line 2)) (1.4.3)\n",
      "Requirement already satisfied: urllib3>=1.25.5 in /opt/conda/lib/python3.11/site-packages (from requests_cache->-r requirments.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: pyrate-limiter<3.0 in /opt/conda/lib/python3.11/site-packages (from requests_ratelimiter->-r requirments.txt (line 3)) (2.10.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (8.1.3)\n",
      "Requirement already satisfied: packaging<25,>=20 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (23.1)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (9.5.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (4.21.12)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (12.0.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (4.6.3)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (3.1.31)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (6.3.2)\n",
      "Requirement already satisfied: watchdog<6,>=2.1.5 in /opt/conda/lib/python3.11/site-packages (from streamlit->-r requirments.txt (line 4)) (5.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->-r requirments.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->-r requirments.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->-r requirments.txt (line 6)) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->-r requirments.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->-r requirments.txt (line 6)) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->-r requirments.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->-r requirments.txt (line 6)) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r requirments.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r requirments.txt (line 4)) (4.17.3)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit->-r requirments.txt (line 4)) (0.12.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4>=4.11.1->yfinance->-r requirments.txt (line 1)) (2.3.2.post1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirments.txt (line 4)) (4.0.10)\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.11/site-packages (from html5lib>=1.1->yfinance->-r requirments.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from html5lib>=1.1->yfinance->-r requirments.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance->-r requirments.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance->-r requirments.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.31->yfinance->-r requirments.txt (line 1)) (2023.5.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r requirments.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit->-r requirments.txt (line 4)) (2.15.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirments.txt (line 4)) (3.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit->-r requirments.txt (line 4)) (2.1.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->-r requirments.txt (line 4)) (0.19.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->-r requirments.txt (line 4)) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, FloatType, IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import date_trunc, avg, round\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from enum import Enum\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "import requests_cache\n",
    "import logging\n",
    "from requests import Session\n",
    "from requests_cache import CacheMixin, SQLiteCache\n",
    "from requests_ratelimiter import LimiterMixin, MemoryQueueBucket\n",
    "from pyrate_limiter import Duration, RequestRate, Limiter\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnNames(Enum):\n",
    "    DATE = \"Date\"\n",
    "    TICKER = \"Ticker\"\n",
    "    OPEN = \"Open\"\n",
    "    HIGH = \"High\"\n",
    "    LOW = \"Low\"\n",
    "    CLOSE = \"Close\"\n",
    "    ADJ_CLOSE = \"Adj Close\"\n",
    "    VOLUME = \"Volume\"\n",
    "    COMPANY_NAME = \"Company\"\n",
    "    INDUSTRY = \"Industry\"\n",
    "    SECTOR = \"Sector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnumPeriod(Enum):\n",
    "    YEAR = \"year\"        \n",
    "    QUARTER = \"quarter\"  \n",
    "    MONTH = \"month\"      \n",
    "    WEEK = \"week\"        \n",
    "    DAY = \"day\"\n",
    "\n",
    "    def get_format(self):\n",
    "        formats = {\n",
    "            \"year\": \"yyyy\",\n",
    "            \"quarter\": \"yyyy-'Q'Q\",\n",
    "            \"month\": \"yyyy-MM\",\n",
    "            \"week\": \"yyyy-'W'ww\",\n",
    "            \"day\": \"yyyy-MM-dd\"\n",
    "        }\n",
    "        return formats[self.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_period_column(period: EnumPeriod, date_column: str):\n",
    "    if period == EnumPeriod.WEEK:\n",
    "        year_col = F.year(date_column).cast(\"string\")\n",
    "        week_col = F.format_string(\"%02d\", F.weekofyear(date_column))\n",
    "        return F.concat(year_col, F.lit(\"-W\"), week_col)\n",
    "    else:\n",
    "        truncated_col = F.date_trunc(period.value, date_column)\n",
    "        return F.date_format(truncated_col, period.get_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_period(start_date: str, period: EnumPeriod, amount: int) -> str:\n",
    "    date_obj = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    \n",
    "    if period == EnumPeriod.DAY:\n",
    "        new_date = date_obj + timedelta(days=amount)\n",
    "    elif period == EnumPeriod.WEEK:\n",
    "        new_date = date_obj + timedelta(weeks=amount)\n",
    "    elif period == EnumPeriod.MONTH:\n",
    "        new_month = (date_obj.month + amount - 1) % 12 + 1\n",
    "        new_year = date_obj.year + (date_obj.month + amount - 1) // 12\n",
    "        last_day_of_new_month = calendar.monthrange(new_year, new_month)[1]\n",
    "        new_day = min(date_obj.day, last_day_of_new_month) \n",
    "        new_date = date_obj.replace(year=new_year, month=new_month, day=new_day)\n",
    "    elif period == EnumPeriod.QUARTER:\n",
    "        new_month = (date_obj.month + amount * 3 - 1) % 12 + 1\n",
    "        new_year = date_obj.year + (date_obj.month + amount * 3 - 1) // 12\n",
    "        last_day_of_new_month = calendar.monthrange(new_year, new_month)[1]\n",
    "        new_day = min(date_obj.day, last_day_of_new_month)\n",
    "        new_date = date_obj.replace(year=new_year, month=new_month, day=new_day)\n",
    "    elif period == EnumPeriod.YEAR:\n",
    "        new_date = date_obj.replace(year=date_obj.year + amount)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown period: {period}\")\n",
    "\n",
    "    return new_date.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedLimiterSession(CacheMixin, LimiterMixin, Session):\n",
    "    pass\n",
    "\n",
    "class App:\n",
    "    _instance = None\n",
    "    _app_name = \"MyStockApp\"\n",
    "    _app_version = \"1.0\"\n",
    "\n",
    "    def __init__(self):\n",
    "        if App._instance is not None:\n",
    "            raise Exception(\"App is a singleton! Use App.get_instance() instead.\")\n",
    "        self.logger = self._setup_logger()\n",
    "        self.logger.info(f\"Starting {App._app_name} version {App._app_version}\")\n",
    "        self.spark = SparkSession.builder.appName(App._app_name).getOrCreate()\n",
    "        self.session = self._setup_session()\n",
    "        App._instance = self\n",
    "\n",
    "    @classmethod\n",
    "    def get_instance(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = App()\n",
    "        return cls._instance\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        logger = logging.getLogger(f\"{App._app_name} Logger\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        if not logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            logger.addHandler(ch)\n",
    "    \n",
    "        return logger\n",
    "\n",
    "\n",
    "    def _setup_session(self):\n",
    "        session = CachedLimiterSession(\n",
    "                    limiter=Limiter(RequestRate(10, Duration.SECOND*5)),\n",
    "                    bucket_class=MemoryQueueBucket,\n",
    "                    backend=SQLiteCache(\"yfinance.cache\"),\n",
    "                )\n",
    "\n",
    "        session.headers['User-agent'] = f\"{App._app_name}/{App._app_version} (Windows NT 10.0; Win64; x64)\"\n",
    "\n",
    "        return session\n",
    "\n",
    "    def get_logger(self):\n",
    "        return self.logger\n",
    "\n",
    "    def get_session(self):\n",
    "        return self.session\n",
    "\n",
    "    def get_spark_session(self):\n",
    "        return self.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NasdaqDF:    \n",
    "    def __init__(self, csv_path, analysis_period):\n",
    "        self.spark = App.get_instance().get_spark_session()\n",
    "        self.logger = App.get_instance().get_logger()\n",
    "        self.session = App.get_instance().get_session()\n",
    "\n",
    "        self.list_nasdaq_path = csv_path\n",
    "        self.stock_schema = self._define_stock_schema()\n",
    "        self.nasdaq_schema = self._define_nasdaq_schema()\n",
    "        self.analysis_period = analysis_period\n",
    "\n",
    "    def _define_stock_schema(self):\n",
    "        \"\"\"Define the schema for the Spark DataFrame.\"\"\"\n",
    "        return StructType([\n",
    "            StructField(ColumnNames.DATE.value, DateType(), True),\n",
    "            StructField(ColumnNames.TICKER.value, StringType(), True),\n",
    "            StructField(ColumnNames.OPEN.value, FloatType(), True),\n",
    "            StructField(ColumnNames.HIGH.value, FloatType(), True),\n",
    "            StructField(ColumnNames.LOW.value, FloatType(), True),\n",
    "            StructField(ColumnNames.CLOSE.value, FloatType(), True),\n",
    "            StructField(ColumnNames.ADJ_CLOSE.value, FloatType(), True),\n",
    "            StructField(ColumnNames.VOLUME.value, IntegerType(), True)\n",
    "        ])\n",
    "\n",
    "    def _define_nasdaq_schema(self):\n",
    "        \"\"\"Define the schema for Nasdaq company information.\"\"\"\n",
    "        return StructType([\n",
    "            StructField(ColumnNames.TICKER.value, StringType(), True),\n",
    "            StructField(ColumnNames.COMPANY_NAME.value, StringType(), True),\n",
    "            StructField(ColumnNames.INDUSTRY.value, StringType(), True),\n",
    "            StructField(ColumnNames.SECTOR.value, StringType(), True)\n",
    "        ])\n",
    "\n",
    "    def load_companies_df(self):\n",
    "        \"\"\"Load the Nasdaq company data with error handling.\"\"\"\n",
    "        self.logger.info(\"Beginning download of companies Dataframe\")\n",
    "        nasdaq_df = None\n",
    "        try:\n",
    "            nasdaq_df = self.spark.read.option(\"delimiter\", \";\").csv(self.list_nasdaq_path, header=True, schema=self.nasdaq_schema)\n",
    "            tickers_df = nasdaq_df.select(ColumnNames.TICKER.value).distinct()\n",
    "            self.tickers = [row[ColumnNames.TICKER.value] for row in tickers_df.collect()]\n",
    "            nasdaq_df = nasdaq_df.select(ColumnNames.COMPANY_NAME.value, ColumnNames.TICKER.value,\n",
    "                                    ColumnNames.INDUSTRY.value, ColumnNames.SECTOR.value)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load companies or tickers in DataFrame: {e}\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            if nasdaq_df is None:\n",
    "                self.logger.warning(\"The nasdaq_df is None. No data loaded.\")\n",
    "            else:\n",
    "                ticker_count = nasdaq_df.select(ColumnNames.TICKER.value).distinct().count()\n",
    "                total_rows = nasdaq_df.count()\n",
    "    \n",
    "                if total_rows == 0:\n",
    "                    self.logger.warning(\"The nasdaq_df is empty after loading. No companies found.\")\n",
    "                elif ticker_count == 0:\n",
    "                    self.logger.warning(\"No distinct tickers found in nasdaq_df.\")\n",
    "                elif total_rows < len(self.tickers):\n",
    "                    self.logger.warning(f\"Loaded DataFrame has fewer rows ({total_rows}) than expected tickers ({len(self.tickers)}).\")\n",
    "                self.logger.info(f\"Companies DataFrame loaded successfully for {ticker_count} distinct tickers\")\n",
    "            return nasdaq_df\n",
    "\n",
    "    def _download_single_ticker(self, ticker):\n",
    "        stock_data = None\n",
    "        try:\n",
    "            stock_data = yf.download(ticker, period=self.analysis_period, rounding=True, session=self.session, progress=False)\n",
    "            \n",
    "            if stock_data.empty:\n",
    "                self.logger.warning(f\"No data returned for ticker: {ticker}\")\n",
    "                return None\n",
    "            \n",
    "            stock_data.columns = stock_data.columns.get_level_values(0)\n",
    "            stock_data.columns.name = None\n",
    "            stock_data.index = stock_data.index.tz_convert('UTC')\n",
    "            stock_data['Date'] = stock_data.index.date\n",
    "            stock_data['Ticker'] = ticker\n",
    "\n",
    "            stock_data = stock_data[['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].reset_index(drop=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Download failed for {ticker}: {e}\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            nan_count = stock_data.isnull().sum().sum()\n",
    "            if nan_count > 0:\n",
    "                self.logger.warning(f\"Found {nan_count} NaN values in the data for ticker: {ticker} at date: {stock_data['Date']}\")\n",
    "            return stock_data  \n",
    "\n",
    "    def load_stock_df(self):\n",
    "        \"\"\"Download data for all tickers with error handling and timeout.\"\"\"\n",
    "        self.logger.info(\"Beginning download of stock Dataframe\")\n",
    "        stock_df = None\n",
    "        combined_rdd = self.spark.sparkContext.emptyRDD()\n",
    "        try:\n",
    "            for ticker in self.tickers:\n",
    "                data = self._download_single_ticker(ticker)\n",
    "                if data is not None:\n",
    "                    rows = [tuple(row) for row in data.to_numpy()]\n",
    "                    rdd = self.spark.sparkContext.parallelize(rows)\n",
    "                    combined_rdd = combined_rdd.union(rdd)\n",
    "            stock_df = self.spark.createDataFrame(combined_rdd, schema=self.stock_schema)\n",
    "            stock_df = stock_df.repartition(len(self.tickers), F.col(ColumnNames.TICKER.value))\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load stock DataFrame: {e}\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            if stock_df is None:\n",
    "                self.logger.warning(\"The stock_df is None. No data loaded.\")\n",
    "            else:\n",
    "                if stock_df.count() < len(self.tickers):\n",
    "                    self.logger.warning(f\"Downloaded data contains fewer rows ({stock_df.count()}) than tickers ({len(self.tickers)})\")\n",
    "                self.logger.info(\"Stock DataFrame loaded successfully.\")\n",
    "            return stock_df\n",
    "\n",
    "    def merge_dataframes(self, stock_df, companies_df):\n",
    "        \"\"\"Merge stock_df and companies_df with error handling.\"\"\"\n",
    "        self.logger.info(\"Merging stock and companies Dataframe\")\n",
    "        merged_df = None\n",
    "        try:\n",
    "            if stock_df is None or companies_df is None:\n",
    "                self.logger.warning(\"DataFrames stock and/or companies not loaded.\")\n",
    "            merged_df = stock_df.join(companies_df, on=ColumnNames.TICKER.value, how='inner')\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to merge DataFrames: {e}\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            if merged_df is not None:\n",
    "                self.logger.info(\"Merged stock and companies DataFrame successfully.\")  \n",
    "            return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 15:40:42,516 - MyStockApp Logger - INFO - Starting MyStockApp version 1.0\n",
      "2024-11-02 15:40:48,275 - MyStockApp Logger - INFO - Beginning download of companies Dataframe\n",
      "2024-11-02 15:40:57,099 - MyStockApp Logger - INFO - Companies DataFrame loaded successfully for 101 distinct tickers\n",
      "2024-11-02 15:40:57,101 - MyStockApp Logger - INFO - Beginning download of stock Dataframe\n",
      "2024-11-02 15:42:12,501 - MyStockApp Logger - INFO - Stock DataFrame loaded successfully.\n",
      "2024-11-02 15:42:12,502 - MyStockApp Logger - INFO - Merging stock and companies Dataframe\n",
      "2024-11-02 15:42:12,594 - MyStockApp Logger - INFO - Merged stock and companies DataFrame successfully.\n"
     ]
    }
   ],
   "source": [
    "app = App.get_instance()\n",
    "nasdaq_data = NasdaqDF(csv_path=\"nasdaq_100_list.csv\", analysis_period=\"1y\")\n",
    "companies_df = nasdaq_data.load_companies_df()\n",
    "stock_df = nasdaq_data.load_stock_df()\n",
    "merged_df = nasdaq_data.merge_dataframes(stock_df, companies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NasdaqAnalysis:\n",
    "    \n",
    "    def __init__(self, stock_df):\n",
    "        self.stock_df = stock_df\n",
    "        self.logger = App.get_instance().get_logger()\n",
    "\n",
    "    def display_stock_bounds(self, ticker: str, num_rows: int = 40):\n",
    "        \"\"\"\n",
    "        Display the first and last `num_rows` rows for the specified ticker in the DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            ticker (str): The stock ticker symbol to filter data.\n",
    "            num_rows (int): Number of rows to display at the start and end of the data. Default is 40.\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Displaying first and last {num_rows} rows for ticker: {ticker}\")\n",
    "            df_filtered = self.stock_df.filter(F.col(ColumnNames.TICKER.value) == ticker)\n",
    "            \n",
    "            print(f\"First {num_rows} rows for ticker: {ticker}\")\n",
    "            df_filtered.orderBy(F.asc(ColumnNames.DATE.value)).show(num_rows)\n",
    "            \n",
    "            print(f\"Last {num_rows} rows for ticker: {ticker}\")\n",
    "            df_filtered.orderBy(F.desc(ColumnNames.DATE.value)).show(num_rows)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error displaying bounds for ticker {ticker}: {e}\")\n",
    "\n",
    "    def count_observations(self):\n",
    "        \"\"\"\n",
    "        Count the total number of observations (rows) in the DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            int: Total number of observations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            total_count = self.stock_df.count()\n",
    "            self.logger.info(f\"Total observations: {total_count}\")\n",
    "            return total_count\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error counting observations: {e}\")\n",
    "            return None\n",
    "\n",
    "    def deduce_data_period(self):\n",
    "        \"\"\"\n",
    "        Calculate both the average and most common data period in days based on date differences.\n",
    "        \n",
    "        Returns:\n",
    "            dict: A dictionary with \"average_period\" as a float and \"most_common_period\" as an integer.\n",
    "        \"\"\"\n",
    "        df = self.stock_df\n",
    "        try:\n",
    "            logger = App.get_instance().get_logger()\n",
    "            logger.info(\"Inferring data period based on date differences.\")\n",
    "    \n",
    "            window_spec = Window.partitionBy(\"Ticker\").orderBy(\"Date\")\n",
    "    \n",
    "            df_with_diff = df.withColumn(\"date_diff\", F.datediff(F.col(\"Date\"), F.lag(\"Date\", 1).over(window_spec)))\n",
    "    \n",
    "            avg_diff = df_with_diff.agg(F.mean(\"date_diff\")).first()[0]\n",
    "    \n",
    "            most_common_diff = (\n",
    "                df_with_diff.groupBy(\"date_diff\")\n",
    "                .count()\n",
    "                .orderBy(F.desc(\"count\"))\n",
    "                .first()\n",
    "            )\n",
    "            most_common_period = most_common_diff[\"date_diff\"]\n",
    "    \n",
    "            logger.info(f\"Inferred average data period: {avg_diff} days\")\n",
    "            logger.info(f\"Inferred most common data period: {most_common_period} days\")\n",
    "            \n",
    "            return {\"average_period\": round(avg_diff, 2), \"most_common_period\": most_common_period}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error inferring data period: {e}\")\n",
    "            return None\n",
    "\n",
    "    def descriptive_statistics(self):\n",
    "        \"\"\"\n",
    "        Display basic descriptive statistics (min, max, mean, standard deviation) for numeric columns.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Calculating descriptive statistics for numeric columns.\")\n",
    "            self.stock_df.describe()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error calculating descriptive statistics: {e}\")\n",
    "\n",
    "    def count_missing_values(self):\n",
    "        \"\"\"\n",
    "        Count the number of missing values for each column in the DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame containing counts of missing values per column.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Counting missing values for each column.\")\n",
    "            missing_df = self.stock_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in self.stock_df.columns])\n",
    "            return missing_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error counting missing values: {e}\")\n",
    "            return None\n",
    "\n",
    "    def calculate_correlation(self):\n",
    "        \"\"\"\n",
    "        Calculate correlations between numeric columns in the DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with tuple keys (column1, column2) and correlation values.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Calculating correlations between numeric columns.\")\n",
    "            numeric_columns = [field.name for field in self.stock_df.schema.fields if isinstance(field.dataType, (FloatType, IntegerType))]\n",
    "            correlations = {}\n",
    "            for i in range(len(numeric_columns)):\n",
    "                for j in range(i + 1, len(numeric_columns)):\n",
    "                    col1, col2 = numeric_columns[i], numeric_columns[j]\n",
    "                    corr_value = self.stock_df.corr(col1, col2)\n",
    "                    correlations[(col1, col2)] = corr_value\n",
    "            return correlations\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error calculating correlations: {e}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 15:42:12,893 - MyStockApp Logger - INFO - Displaying first and last 40 rows for ticker: AAPL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 40 rows for ticker: AAPL\n",
      "+----------+------+------+------+------+------+---------+---------+\n",
      "|      Date|Ticker|  Open|  High|   Low| Close|Adj Close|   Volume|\n",
      "+----------+------+------+------+------+------+---------+---------+\n",
      "|2023-11-02|  AAPL|175.52|177.78|175.46|177.57|   176.67| 77334800|\n",
      "|2023-11-03|  AAPL|174.24|176.82|173.35|176.65|   175.75| 79763700|\n",
      "|2023-11-06|  AAPL|176.38|179.43|176.21|179.23|   178.32| 63841300|\n",
      "|2023-11-07|  AAPL|179.18|182.44|178.97|181.82|   180.89| 70530000|\n",
      "|2023-11-08|  AAPL|182.35|183.45|181.59|182.89|   181.96| 49340300|\n",
      "|2023-11-09|  AAPL|182.96|184.12|181.81|182.41|   181.48| 53763500|\n",
      "|2023-11-10|  AAPL|183.97|186.57|183.53| 186.4|    185.7| 66133400|\n",
      "|2023-11-13|  AAPL|185.82|186.03|184.21| 184.8|    184.1| 43627500|\n",
      "|2023-11-14|  AAPL| 187.7|188.11| 186.3|187.44|   186.73| 60108400|\n",
      "|2023-11-15|  AAPL|187.85| 189.5|187.78|188.01|    187.3| 53790500|\n",
      "|2023-11-16|  AAPL|189.57|190.96|188.65|189.71|   188.99| 54412900|\n",
      "|2023-11-17|  AAPL|190.25|190.38|188.57|189.69|   188.97| 50922700|\n",
      "|2023-11-20|  AAPL|189.89|191.91|189.88|191.45|   190.73| 46505100|\n",
      "|2023-11-21|  AAPL|191.41|191.52|189.74|190.64|   189.92| 38134500|\n",
      "|2023-11-22|  AAPL|191.49|192.93|190.83|191.31|   190.59| 39617700|\n",
      "|2023-11-24|  AAPL|190.87| 190.9|189.25|189.97|   189.25| 24048300|\n",
      "|2023-11-27|  AAPL|189.92|190.67| 188.9|189.79|   189.07| 40552600|\n",
      "|2023-11-28|  AAPL|189.78|191.08| 189.4| 190.4|   189.68| 38415400|\n",
      "|2023-11-29|  AAPL| 190.9|192.09|188.97|189.37|   188.65| 43014200|\n",
      "|2023-11-30|  AAPL|189.84|190.32|188.19|189.95|   189.23| 48794400|\n",
      "|2023-12-01|  AAPL|190.33|191.56|189.23|191.24|   190.52| 45679300|\n",
      "|2023-12-04|  AAPL|189.98|190.05|187.45|189.43|   188.71| 43389500|\n",
      "|2023-12-05|  AAPL|190.21| 194.4|190.18|193.42|   192.69| 66628400|\n",
      "|2023-12-06|  AAPL|194.45|194.76|192.11|192.32|   191.59| 41089700|\n",
      "|2023-12-07|  AAPL|193.63| 195.0|193.59|194.27|   193.54| 47477700|\n",
      "|2023-12-08|  AAPL| 194.2|195.99|193.67|195.71|   194.97| 53377300|\n",
      "|2023-12-11|  AAPL|193.11|193.49|191.42|193.18|   192.45| 60943700|\n",
      "|2023-12-12|  AAPL|193.08|194.72|191.72|194.71|   193.97| 52696900|\n",
      "|2023-12-13|  AAPL|195.09| 198.0|194.85|197.96|   197.21| 70404200|\n",
      "|2023-12-14|  AAPL|198.02|199.62|196.16|198.11|   197.36| 66831600|\n",
      "|2023-12-15|  AAPL|197.53| 198.4| 197.0|197.57|   196.82|128256700|\n",
      "|2023-12-18|  AAPL|196.09|196.63|194.39|195.89|   195.15| 55751900|\n",
      "|2023-12-19|  AAPL|196.16|196.95|195.89|196.94|    196.2| 40714100|\n",
      "|2023-12-20|  AAPL| 196.9|197.68|194.83|194.83|   194.09| 52242800|\n",
      "|2023-12-21|  AAPL| 196.1|197.08| 193.5|194.68|   193.94| 46482500|\n",
      "|2023-12-22|  AAPL|195.18|195.41|192.97| 193.6|   192.87| 37122800|\n",
      "|2023-12-26|  AAPL|193.61|193.89|192.83|193.05|   192.32| 28919300|\n",
      "|2023-12-27|  AAPL|192.49| 193.5|191.09|193.15|   192.42| 48087700|\n",
      "|2023-12-28|  AAPL|194.14|194.66|193.17|193.58|   192.85| 34049900|\n",
      "|2023-12-29|  AAPL| 193.9| 194.4|191.73|192.53|    191.8| 42628800|\n",
      "+----------+------+------+------+------+------+---------+---------+\n",
      "only showing top 40 rows\n",
      "\n",
      "Last 40 rows for ticker: AAPL\n",
      "+----------+------+------+------+------+------+---------+---------+\n",
      "|      Date|Ticker|  Open|  High|   Low| Close|Adj Close|   Volume|\n",
      "+----------+------+------+------+------+------+---------+---------+\n",
      "|2024-11-01|  AAPL|220.97|225.35|220.27|222.91|   222.91| 65242200|\n",
      "|2024-10-31|  AAPL|229.34|229.83|225.37|225.91|   225.91| 64370100|\n",
      "|2024-10-30|  AAPL|232.61|233.47|229.55| 230.1|    230.1| 47070900|\n",
      "|2024-10-29|  AAPL| 233.1|234.33|232.32|233.67|   233.67| 35417200|\n",
      "|2024-10-28|  AAPL|233.32|234.73|232.55| 233.4|    233.4| 36087100|\n",
      "|2024-10-25|  AAPL|229.74|233.22|229.57|231.41|   231.41| 38802300|\n",
      "|2024-10-24|  AAPL|229.98|230.82|228.41|230.57|   230.57| 31109500|\n",
      "|2024-10-23|  AAPL|234.08|235.14|227.76|230.76|   230.76| 52287000|\n",
      "|2024-10-22|  AAPL|233.89|236.22| 232.6|235.86|   235.86| 38846600|\n",
      "|2024-10-21|  AAPL|234.45|236.85|234.45|236.48|   236.48| 36254500|\n",
      "|2024-10-18|  AAPL|236.18|236.18|234.01| 235.0|    235.0| 46431500|\n",
      "|2024-10-17|  AAPL|233.43|233.85|230.52|232.15|   232.15| 32993800|\n",
      "|2024-10-16|  AAPL| 231.6|232.12|229.84|231.78|   231.78| 34082200|\n",
      "|2024-10-15|  AAPL|233.61|237.49|232.37|233.85|   233.85| 64751400|\n",
      "|2024-10-14|  AAPL| 228.7|231.73| 228.6| 231.3|    231.3| 39882100|\n",
      "|2024-10-11|  AAPL| 229.3|229.41|227.34|227.55|   227.55| 31759200|\n",
      "|2024-10-10|  AAPL|227.78| 229.5|227.17|229.04|   229.04| 28183500|\n",
      "|2024-10-09|  AAPL|225.23|229.75|224.83|229.54|   229.54| 33591100|\n",
      "|2024-10-08|  AAPL| 224.3|225.98|223.25|225.77|   225.77| 31855700|\n",
      "|2024-10-07|  AAPL| 224.5|225.69|221.33|221.69|   221.69| 39505400|\n",
      "|2024-10-04|  AAPL| 227.9| 228.0|224.13| 226.8|    226.8| 37245100|\n",
      "|2024-10-03|  AAPL|225.14|226.81|223.32|225.67|   225.67| 34044200|\n",
      "|2024-10-02|  AAPL|225.89|227.37|223.02|226.78|   226.78| 32880600|\n",
      "|2024-10-01|  AAPL|229.52|229.65|223.74|226.21|   226.21| 63285000|\n",
      "|2024-09-30|  AAPL|230.04| 233.0|229.65| 233.0|    233.0| 54541900|\n",
      "|2024-09-27|  AAPL|228.46|229.52| 227.3|227.79|   227.79| 34026000|\n",
      "|2024-09-26|  AAPL| 227.3| 228.5|225.41|227.52|   227.52| 36636700|\n",
      "|2024-09-25|  AAPL|224.93|227.29|224.02|226.37|   226.37| 42308700|\n",
      "|2024-09-24|  AAPL|228.65|229.35|225.73|227.37|   227.37| 43556100|\n",
      "|2024-09-23|  AAPL|227.34|229.45|225.81|226.47|   226.47| 54146000|\n",
      "|2024-09-20|  AAPL|229.97|233.09|227.62| 228.2|    228.2|318679900|\n",
      "|2024-09-19|  AAPL|224.99|229.82|224.63|228.87|   228.87| 66781300|\n",
      "|2024-09-18|  AAPL|217.55|222.71|217.54|220.69|   220.69| 59894900|\n",
      "|2024-09-17|  AAPL|215.75| 216.9| 214.5|216.79|   216.79| 45519300|\n",
      "|2024-09-16|  AAPL|216.54|217.22|213.92|216.32|   216.32| 59357400|\n",
      "|2024-09-13|  AAPL|223.58|224.04|221.91| 222.5|    222.5| 36766600|\n",
      "|2024-09-12|  AAPL| 222.5|223.55|219.82|222.77|   222.77| 37498200|\n",
      "|2024-09-11|  AAPL|221.46|223.09|217.89|222.66|   222.66| 44587100|\n",
      "|2024-09-10|  AAPL|218.92|221.48|216.73|220.11|   220.11| 51591000|\n",
      "|2024-09-09|  AAPL|220.82|221.27|216.71|220.91|   220.91| 67180000|\n",
      "+----------+------+------+------+------+------+---------+---------+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 15:43:07,389 - MyStockApp Logger - INFO - Total observations: 25452\n",
      "2024-11-02 15:43:07,392 - MyStockApp Logger - INFO - Inferring data period based on date differences.\n",
      "2024-11-02 15:43:46,115 - MyStockApp Logger - INFO - Inferred average data period: 1.454183266932271 days\n",
      "2024-11-02 15:43:46,116 - MyStockApp Logger - INFO - Inferred most common data period: 1 days\n",
      "2024-11-02 15:43:46,117 - MyStockApp Logger - ERROR - Error inferring data period: Invalid argument, not a string or column: 1.454183266932271 of type <class 'float'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.\n",
      "2024-11-02 15:43:46,118 - MyStockApp Logger - INFO - Calculating descriptive statistics for numeric columns.\n",
      "2024-11-02 15:43:46,398 - MyStockApp Logger - INFO - Counting missing values for each column.\n",
      "2024-11-02 15:43:46,505 - MyStockApp Logger - INFO - Calculating correlations between numeric columns.\n"
     ]
    }
   ],
   "source": [
    "analysis = NasdaqAnalysis(stock_df)\n",
    "analysis.display_stock_bounds('AAPL')\n",
    "nb_count = analysis.count_observations()\n",
    "dict_period = analysis.deduce_data_period()\n",
    "df_stats = analysis.descriptive_statistics()\n",
    "missing_df = analysis.count_missing_values()\n",
    "correlations = analysis.calculate_correlation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataFrameOperations:\n",
    "    def __init__(self, logger: logging.Logger, stock_df: DataFrame):\n",
    "        self.logger = logger\n",
    "        self.stock_df = stock_df\n",
    "\n",
    "    def df_periodic_change(self, period: EnumPeriod, column: ColumnNames) -> DataFrame:\n",
    "        self.logger.info(\"Calculating periodic change for column: %s with period: %s\", column.value, period.value)\n",
    "\n",
    "        if period == EnumPeriod.DAY:\n",
    "            window_spec = Window.partitionBy(ColumnNames.TICKER.value).orderBy(ColumnNames.DATE.value)\n",
    "            daily_change_df = self.stock_df.withColumn(f\"daily_change_{column.value}\", \n",
    "                round(F.col(column.value) - F.lag(column.value, 1).over(window_spec), 2))\n",
    "            return daily_change_df.select(ColumnNames.TICKER.value, ColumnNames.DATE.value, f\"daily_change_{column.value}\")\n",
    "\n",
    "        period_col = format_period_column(period, ColumnNames.DATE.value)\n",
    "\n",
    "        return (\n",
    "            self.stock_df\n",
    "            .groupBy(ColumnNames.TICKER.value, period_col.alias(f\"{period.value}_period\"))\n",
    "            .agg(\n",
    "                F.first(column.value).alias(f\"first_{column.value}\"),\n",
    "                F.last(column.value).alias(f\"last_{column.value}\")\n",
    "            )\n",
    "            .withColumn(f\"{period.value}_change_{column.value}\", \n",
    "                        round(F.col(f\"last_{column.value}\") - F.col(f\"first_{column.value}\"), 2))\n",
    "            .select(ColumnNames.TICKER.value, f\"{period.value}_period\", f\"{period.value}_change_{column.value}\")\n",
    "        )\n",
    "\n",
    "    def avg_open_close_by_period(self, period: EnumPeriod) -> DataFrame:\n",
    "        self.logger.info(\"Calculating average Open and Close by period: %s\", period.value)\n",
    "\n",
    "        period_col = format_period_column(period, ColumnNames.DATE.value)\n",
    "\n",
    "        return (\n",
    "            self.stock_df\n",
    "            .groupBy(ColumnNames.TICKER.value, period_col.alias(f\"{period.value}_period\"))\n",
    "            .agg(\n",
    "                round(F.avg(ColumnNames.OPEN.value), 2).alias(\"avg_open\"), \n",
    "                round(F.avg(ColumnNames.CLOSE.value), 2).alias(\"avg_close\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def calculate_return_rate(self, period: EnumPeriod, column: str) -> DataFrame:\n",
    "        self.logger.info(\"Calculating return rate for column: %s with period: %s\", column, period.value)\n",
    "    \n",
    "        if period == EnumPeriod.DAY:\n",
    "            window_spec = Window.partitionBy(ColumnNames.TICKER.value).orderBy(ColumnNames.Date.value)\n",
    "            daily_return_df = self.stock_df.withColumn(\n",
    "                f\"{period.value}_return_rate\",\n",
    "                F.round(((F.col(column.value) - F.lag(column.value, 1).over(window_spec)) / F.lag(column.value, 1).over(window_spec)) * 100, 2)\n",
    "            )\n",
    "            return daily_return_df.select(ColumnNames.TICKER.value, ColumnNames.DATE.value, f\"{period.value}_return_rate\")\n",
    "        \n",
    "        period_col = format_period_column(period, ColumnNames.DATE.value)\n",
    "        \n",
    "        return_rate_df = (\n",
    "            self.stock_df\n",
    "            .groupBy(ColumnNames.TICKER.value, period_col.alias(f\"{period.value}_period\"))\n",
    "            .agg(\n",
    "                F.first(column.value).alias(f\"first_{column.value}\"),\n",
    "                F.last(column.value).alias(f\"last_{column.value}\")\n",
    "            )\n",
    "            .withColumn(\n",
    "                f\"{period.value}_return_rate\",\n",
    "                F.round(((F.col(f\"last_{column.value}\") - F.col(f\"first_{column.value}\")) / F.col(f\"first_{column.value}\")) * 100, 2)\n",
    "            )\n",
    "            .select(ColumnNames.TICKER.value, f\"{period.value}_period\", f\"{period.value}_return_rate\")\n",
    "        )\n",
    "        \n",
    "        return return_rate_df\n",
    "\n",
    "    def calculate_daily_return(self) -> DataFrame:\n",
    "        self.logger.info(\"Calculating daily return for each stock.\")\n",
    "        \n",
    "        new_stock_df = self.stock_df.withColumn(\"daily_return\", \n",
    "            round(100 * ((F.col(ColumnNames.CLOSE.value) - F.col(ColumnNames.OPEN.value)) / F.col(ColumnNames.OPEN.value)), 4)\n",
    "        )\n",
    "\n",
    "        self.stock_df = new_stock_df\n",
    "        \n",
    "        return new_stock_df\n",
    "\n",
    "    def avg_daily_return_by_period(self, period: EnumPeriod):\n",
    "        self.logger.info(\"Calculating average daily return for each stock within the period.\")\n",
    "        \n",
    "        if \"daily_return\" not in self.stock_df.columns:\n",
    "            self.logger.warning(\"daily_return column not found. Calculating daily returns first.\")\n",
    "            self.calculate_daily_return()\n",
    "\n",
    "        period_col = format_period_column(period, ColumnNames.DATE.value)\n",
    "\n",
    "        return (self.stock_df\n",
    "                .groupBy(ColumnNames.TICKER.value, period_col.alias(f\"{period.value}_period\"))\n",
    "                .agg(round(F.avg(\"daily_return\"), 4).alias(\"avg_daily_return\"))\n",
    "               )\n",
    "\n",
    "    def stocks_with_highest_daily_return(self, daily_return_df: DataFrame, top_n: int = 5) -> DataFrame:\n",
    "        self.logger.info(\"Finding stocks with the highest daily return.\")\n",
    "\n",
    "        if \"daily_return\" not in self.stock_df.columns:\n",
    "            self.logger.warning(\"daily_return column not found. Calculating daily returns first.\")\n",
    "            self.calculate_daily_return()\n",
    "\n",
    "        return daily_return_df.orderBy(F.desc(\"daily_return\")).limit(top_n)\n",
    "\n",
    "    def calculate_moving_average(self, column: str, num_days: int) -> DataFrame:\n",
    "        self.logger.info(f\"Calculating moving average on {num_days} days period.\")\n",
    "\n",
    "        moving_avg_window = Window.partitionBy(ColumnNames.TICKER.value).orderBy(ColumnNames.DATE.value).rowsBetween(-num_days + 1, 0)\n",
    "        count_col = F.count(F.col(column)).over(moving_avg_window)\n",
    "    \n",
    "        return (self.stock_df\n",
    "                .withColumn(f\"{column}_moving_avg_{num_days}_days\",\n",
    "                F.when(count_col == num_days, F.avg(F.col(column)).over(moving_avg_window)).otherwise(None))\n",
    "               )\n",
    "\n",
    "    def calculate_correlation_pairs(self) -> DataFrame:\n",
    "        self.logger.info(\"Calculating correlations between all possible ticker pairs.\")\n",
    "\n",
    "        if \"daily_return\" not in self.stock_df.columns:\n",
    "            self.logger.warning(\"daily_return column not found. Calculating daily returns first.\")\n",
    "            self.calculate_daily_return()\n",
    "\n",
    "        daily_return_df_a = self.stock_df.alias(\"a\")\n",
    "        daily_return_df_b = self.stock_df.alias(\"b\")\n",
    "\n",
    "        joined_df = (\n",
    "            daily_return_df_a.join(daily_return_df_b, (F.col(\"a.Date\") == F.col(\"b.Date\")) & \n",
    "                                                     (F.col(\"a.Ticker\") < F.col(\"b.Ticker\")))\n",
    "        )\n",
    "\n",
    "        correlation_df = (\n",
    "            joined_df.groupBy(\"a.Ticker\", \"b.Ticker\")\n",
    "                     .agg(F.corr(\"a.daily_return\", \"b.daily_return\").alias(\"correlation\"))\n",
    "                     .filter(F.col(\"correlation\").isNotNull())\n",
    "                     .orderBy(F.desc(\"correlation\"))\n",
    "        )\n",
    "\n",
    "        return correlation_df\n",
    "\n",
    "    def calculate_periodic_return(self, period: str, price_column: str = \"Close\") -> DataFrame:\n",
    "        period_col = format_period_column(period, ColumnNames.DATE.value)\n",
    "        return_df = (stock_df\n",
    "                     .groupBy(ColumnNames.TICKER.value, period_col.alias(f\"{period.value}_period\"))\n",
    "                     .agg(\n",
    "                         F.first(price_column).alias(\"first_price\"),\n",
    "                         F.last(price_column).alias(\"last_price\")\n",
    "                     )\n",
    "                     .withColumn(f\"{period}_return_rate\", \n",
    "                                 (F.col(\"last_price\") - F.col(\"first_price\")) / F.col(\"first_price\") * 100)\n",
    "                     .select(\"Ticker\", period_col, f\"{period}_return_rate\")\n",
    "                    )\n",
    "    \n",
    "        return return_df\n",
    "\n",
    "    def calculate_best_return_rate(self, start_date: str, period: EnumPeriod, column: ColumnNames):\n",
    "        self.logger.info(\"Calculating best return rate from %s for period: %s\", start_date, period.value)\n",
    "    \n",
    "        end_date = add_period(start_date, period, 1)\n",
    "    \n",
    "        filtered_df = self.stock_df.filter((F.col(ColumnNames.DATE.value) >= start_date) & (F.col(ColumnNames.DATE.value) <= end_date))\n",
    "    \n",
    "        return_rate_df = filtered_df.groupBy(ColumnNames.TICKER.value).agg(\n",
    "            F.first(column.value).alias(\"first_price\"),\n",
    "            F.last(column.value).alias(\"last_price\")\n",
    "        ).withColumn(\n",
    "            \"return_rate\",\n",
    "            F.round(((F.col(\"last_price\") - F.col(\"first_price\")) / F.col(\"first_price\")) * 100, 2)\n",
    "        )\n",
    "    \n",
    "        return return_rate_df.orderBy(F.col(\"return_rate\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 15:47:51,027 - MyStockApp Logger - INFO - Calculating periodic change for column: Close with period: month\n",
      "2024-11-02 15:47:51,056 - MyStockApp Logger - INFO - Calculating average Open and Close by period: month\n",
      "2024-11-02 15:47:51,069 - MyStockApp Logger - INFO - Calculating return rate for column: ColumnNames.CLOSE with period: month\n",
      "2024-11-02 15:47:51,097 - MyStockApp Logger - INFO - Calculating daily return for each stock.\n",
      "2024-11-02 15:47:51,106 - MyStockApp Logger - INFO - Finding stocks with the highest daily return.\n",
      "2024-11-02 15:47:51,112 - MyStockApp Logger - INFO - Calculating average daily return for each stock within the period.\n",
      "2024-11-02 15:47:51,124 - MyStockApp Logger - INFO - Calculating moving average on 5 days period.\n",
      "2024-11-02 15:47:51,196 - MyStockApp Logger - INFO - Calculating correlations between all possible ticker pairs.\n",
      "2024-11-02 15:47:51,294 - MyStockApp Logger - INFO - Calculating best return rate from 2024-01-03 for period: month\n"
     ]
    }
   ],
   "source": [
    "df_operations = DataFrameOperations(App.get_instance().get_logger(), stock_df)\n",
    "period = EnumPeriod.MONTH\n",
    "column = ColumnNames.CLOSE\n",
    "df_periodic_change = df_operations.df_periodic_change(period, column)\n",
    "df_avg_close = df_operations.avg_open_close_by_period(period)\n",
    "df_month_return = df_operations.calculate_return_rate(period, column)\n",
    "df_daily_return = df_operations.calculate_daily_return()\n",
    "df_highest_daily_return = df_operations.stocks_with_highest_daily_return(df_daily_return)\n",
    "df_avg_daily_return = df_operations.avg_daily_return_by_period(period)\n",
    "df_mvg_avg = df_operations.calculate_moving_average(ColumnNames.OPEN.value, 5)\n",
    "df_correlations = df_operations.calculate_correlation_pairs()\n",
    "df_best_return_rate = df_operations.calculate_best_return_rate(\"2024-01-03\", EnumPeriod.MONTH, ColumnNames.CLOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 15:47:51,324 - MyStockApp Logger - INFO - Beginning download of companies Dataframe\n",
      "2024-11-02 15:47:52,017 - MyStockApp Logger - INFO - Companies DataFrame loaded successfully for 101 distinct tickers\n",
      "2024-11-02 15:47:52,018 - MyStockApp Logger - INFO - Beginning download of stock Dataframe\n",
      "2024-11-02 15:48:16,301 - MyStockApp Logger - INFO - Stock DataFrame loaded successfully.\n",
      "2024-11-02 15:48:16,303 - MyStockApp Logger - INFO - Merging stock and companies Dataframe\n",
      "2024-11-02 15:48:16,309 - MyStockApp Logger - INFO - Merged stock and companies DataFrame successfully.\n",
      "2024-11-02 15:48:16.310 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.582 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/conda/lib/python3.11/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-11-02 15:48:16.583 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.583 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.584 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16,584 - MyStockApp Logger - INFO - Calculating descriptive statistics for numeric columns.\n",
      "2024-11-02 15:48:16.614 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.615 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.617 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.618 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.619 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.620 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.621 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.621 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.623 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.623 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.689 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.689 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.691 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.691 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.694 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-11-02 15:48:16.696 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NasdaqApp:\n",
    "    def __init__(self, csv_path):\n",
    "        self.nasdaq_data = NasdaqDF(csv_path=csv_path, analysis_period=\"1y\")\n",
    "        self.companies_df = self.nasdaq_data.load_companies_df()\n",
    "        self.stock_df = self.nasdaq_data.load_stock_df()\n",
    "        self.merged_df = self.nasdaq_data.merge_dataframes(self.stock_df, self.companies_df)\n",
    "        self.analysis = NasdaqAnalysis(self.stock_df)\n",
    "\n",
    "    def run(self):\n",
    "        st.title(\"NASDAQ Analysis App\")\n",
    "\n",
    "        # General Statistics\n",
    "        st.subheader(\"General Statistics\")\n",
    "        st.write(self.analysis.descriptive_statistics())\n",
    "        \n",
    "        # Buttons for different analyses\n",
    "        if st.button(\"Count Observations\"):\n",
    "            st.write(\"Number of Observations:\", self.analysis.count_observations())\n",
    "\n",
    "        if st.button(\"Count Missing Values\"):\n",
    "            st.write(self.analysis.count_missing_values())\n",
    "\n",
    "        if st.button(\"Calculate Correlation\"):\n",
    "            correlations = self.analysis.calculate_correlation()\n",
    "            st.write(correlations)\n",
    "\n",
    "        if st.button(\"Display Stock Bounds (e.g., AAPL)\"):\n",
    "            stock_ticker = st.text_input(\"Enter Stock Ticker\", value='AAPL')\n",
    "            st.write(self.analysis.display_stock_bounds(stock_ticker))\n",
    "\n",
    "        # Visualizations with Seaborn or Matplotlib\n",
    "        if st.button(\"Show Daily Returns\"):\n",
    "            df_daily_return = self.analysis.calculate_daily_return()\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.lineplot(data=df_daily_return, x='date', y='daily_return', label='Daily Return')\n",
    "            plt.title('Daily Return')\n",
    "            plt.xlabel('Date')\n",
    "            plt.ylabel('Return Rate')\n",
    "            st.pyplot(plt)\n",
    "\n",
    "        # Add more buttons for other analyses and visualizations as needed\n",
    "\n",
    "# To run the app\n",
    "if __name__ == \"__main__\":\n",
    "    app = NasdaqApp(csv_path=\"nasdaq_100_list.csv\")\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_instance = App.get_instance()\n",
    "logger = app_instance.get_logger()\n",
    "\n",
    "st.title(\"Stock Analysis Dashboard\")\n",
    "\n",
    "if st.button(\"Load Data\"):\n",
    "\n",
    "        if stock_df is not None and companies_df is not None:\n",
    "            merged_df = nasdaq_df.merge_dataframes(stock_df, companies_df)\n",
    "            st.success(\"Data loaded successfully!\")\n",
    "\n",
    "            # Example analysis: Display first few rows of merged data\n",
    "            st.subheader(\"Merged Data Preview\")\n",
    "            st.write(merged_df.limit(10).toPandas())\n",
    "\n",
    "            # Create an instance of NasdaqAnalysis\n",
    "            analysis = NasdaqAnalysis(merged_df)\n",
    "\n",
    "            # Display statistics\n",
    "            if st.button(\"Show Descriptive Statistics\"):\n",
    "                stats = analysis.descriptive_statistics()\n",
    "                st.write(stats)\n",
    "\n",
    "            # Functionality for displaying stock bounds\n",
    "            ticker = st.selectbox(\"Select Ticker\", options=nazdaq_df.tickers)\n",
    "            if st.button(\"Show Stock Bounds\"):\n",
    "                analysis.display_stock_bounds(ticker)\n",
    "\n",
    "            # Calculate correlation pairs\n",
    "            if st.button(\"Calculate Correlation Pairs\"):\n",
    "                correlation_df = analysis.calculate_correlation_pairs()\n",
    "                st.write(correlation_df.toPandas())\n",
    "\n",
    "        else:\n",
    "            st.error(\"Failed to load stock data.\")\n",
    "    else:\n",
    "        st.warning(\"Please provide both CSV path and analysis period.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
